"""SQL Agent è¯„ä¼°æ¡†æ¶ - LLMè¯„åˆ†ç‰ˆæœ¬"""

import os
import json
import re
import requests
import asyncio
import logging
from pathlib import Path
from dotenv import load_dotenv
import pymysql
from jinja2 import Environment, FileSystemLoader
from agents.mock_agent import mock_agent


def get_template_env():
    """è·å–Jinja2æ¨¡æ¿ç¯å¢ƒ"""
    return Environment(loader=FileSystemLoader('prompts'))


def scan_datasets():
    """è‡ªåŠ¨æ‰«ædatasetsç›®å½•ä¸‹çš„æ•°æ®é›†"""
    datasets_dir = Path("datasets")
    datasets = []
    
    if not datasets_dir.exists():
        logging.error("datasetsç›®å½•ä¸å­˜åœ¨")
        return datasets
    
    # éå†datasetsä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for dataset in datasets_dir.iterdir():
        if dataset.is_dir():
            dataset_name = dataset.name
            schema_file = dataset / "schema.sql"
            data_file = dataset / "data.sql"
            questions_file = dataset / "questions.md"
            
            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if questions_file.exists():
                datasets.append({
                    "name": dataset_name,
                    "schema_file": str(schema_file),
                    "data_file": str(data_file),
                    "questions_file": str(questions_file)
                })
                logging.info(f"å‘ç°æ•°æ®é›†: {dataset_name}")
            else:
                logging.warning(f"æ•°æ®é›† {dataset_name} ç¼ºå°‘ questions.md æ–‡ä»¶")
    
    return datasets


def setup_database_with_connection(conn, schema_file, data_file):
    """ä½¿ç”¨å·²æœ‰è¿æ¥è®¾ç½®æ•°æ®åº“ç¯å¢ƒ"""
    cursor = conn.cursor()
    
    # æ‰§è¡Œschema
    with open(schema_file, 'r') as f:
        schema_sql = f.read()
        statements = schema_sql.split(';')
        for stmt in statements:
            stmt = stmt.strip()
            if stmt:
                try:
                    cursor.execute(stmt)
                    logging.debug(f"æ‰§è¡ŒSQL: {stmt[:50]}...")
                except Exception as e:
                    if "already exists" in str(e) or "Table" in str(e):
                        logging.debug(f"è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡: {stmt[:50]}...")
                    else:
                        logging.error(f"SQLæ‰§è¡Œå¤±è´¥: {e}")
    
    # æ‰§è¡Œdata
    with open(data_file, 'r') as f:
        data_sql = f.read()
        statements = data_sql.split(';')
        for stmt in statements:
            stmt = stmt.strip()
            if stmt.upper().startswith('INSERT'):
                try:
                    cursor.execute(stmt)
                    logging.debug(f"æ’å…¥æ•°æ®: {stmt[:30]}...")
                except Exception as e:
                    if "Duplicate entry" in str(e):
                        logging.debug("æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    else:
                        logging.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
    
    conn.commit()
    cursor.close()
    logging.debug("æ•°æ®åº“ç¯å¢ƒå‡†å¤‡å®Œæˆ")

def setup_database(schema_file, data_file):
    """è¿æ¥æ•°æ®åº“å¹¶æ„å»ºç¯å¢ƒ"""
    logging.info("è¿æ¥æ•°æ®åº“...")
    
    config = {
        'host': os.getenv('DATABASE_HOST', 'localhost'),
        'port': int(os.getenv('DATABASE_PORT', 3306)),
        'user': os.getenv('DATABASE_USER', 'root'),
        'password': os.getenv('DATABASE_PASSWORD', ''),
        'database': os.getenv('DATABASE_NAME', 'sql_eval_test')
    }
    
    conn = pymysql.connect(**config)
    logging.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {config['host']}:{config['port']}")
    
    # åˆ›å»ºè¡¨å’Œæ•°æ®
    cursor = conn.cursor()
    
    # åˆ›å»ºè¡¨ï¼ˆæ”¯æŒé‡å¤æ‰§è¡Œï¼‰
    with open(schema_file, 'r') as f:
        schema_sql = f.read()
        # é€æ¡æ‰§è¡ŒSQLï¼Œå¿½ç•¥å·²å­˜åœ¨çš„é”™è¯¯
        statements = schema_sql.split(';')
        for stmt in statements:
            stmt = stmt.strip()
            if stmt:
                try:
                    cursor.execute(stmt)
                    logging.debug(f"æ‰§è¡ŒSQL: {stmt[:50]}...")
                except Exception as e:
                    if "already exists" in str(e) or "Table" in str(e):
                        logging.debug(f"è¡¨å·²å­˜åœ¨ï¼Œè·³è¿‡: {stmt[:50]}...")
                    else:
                        logging.error(f"SQLæ‰§è¡Œå¤±è´¥: {e}")
    
    # æ’å…¥æ•°æ®
    with open(data_file, 'r') as f:
        data_sql = f.read()
        statements = data_sql.split(';')
        for stmt in statements:
            stmt = stmt.strip()
            if stmt.upper().startswith('INSERT'):
                try:
                    cursor.execute(stmt)
                    logging.debug(f"æ’å…¥æ•°æ®: {stmt[:30]}...")
                except Exception as e:
                    if "Duplicate entry" in str(e):
                        logging.debug("æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    else:
                        logging.error(f"æ’å…¥æ•°æ®å¤±è´¥: {e}")
    
    conn.commit()
    cursor.close()
    
    logging.info("æ•°æ®åº“ç¯å¢ƒå‡†å¤‡å®Œæˆ")
    return conn


def parse_questions(md_file):
    """è§£æ questions.md æ–‡ä»¶"""
    logging.info(f"è§£æé—®é¢˜æ–‡ä»¶: {md_file}")
    
    with open(md_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    sections = content.split('## é—®é¢˜')[1:]
    questions = []
    
    for section in sections:
        # æå–æ ‡é¢˜
        title_match = re.search(r'(\d+)[ï¼š:]\s*([^\n]+)', section)
        title = title_match.group(2) if title_match else "æœªå‘½åé—®é¢˜"
        
        # æå–SQL
        sql_match = re.search(r'\*\*SQL:\*\*\s*`([^`]+)`', section)
        sql = sql_match.group(1) if sql_match else ""
        
        # æå–æœŸæœ›ç»“æœ
        result_match = re.search(r'\*\*æœŸæœ›ç»“æœ:\*\*\s*([^\n]+)', section)
        expected = result_match.group(1) if result_match else ""
        
        # æå–è¯„åˆ†è§„åˆ™
        scoring_match = re.search(r'\*\*è¯„åˆ†:\*\*.*?(.*?)(?=---|$)', section, re.DOTALL)
        scoring_rules = scoring_match.group(1).strip() if scoring_match else ""
        
        if sql:
            questions.append({
                'title': title,
                'sql': sql.strip(),
                'expected': expected,
                'scoring_rules': scoring_rules
            })
    
    logging.info(f"è§£æåˆ° {len(questions)} ä¸ªé—®é¢˜")
    return questions


# Agentç›¸å…³ä»£ç å·²æŠ½å–åˆ° agents/ ç›®å½•


async def evaluate_with_llm(agent_response, expected_result, scoring_rules, sql_query):
    """ä½¿ç”¨LLMè¯„åˆ†"""
    if not agent_response:
        return {"score": 0, "explanation": "Agentæ²¡æœ‰ç»™å‡ºå›ç­”"}
    
    agent_optimization = agent_response.get('optimization', '')
    agent_reasoning = agent_response.get('reasoning', '')
    
    # ä½¿ç”¨Jinja2æ¸²æŸ“æ¨¡æ¿
    env = get_template_env()
    template = env.get_template('evaluation.tpl')
    
    prompt = template.render(
        sql_query=sql_query,
        db_context='ç”¨æˆ·è¡¨(users)åŒ…å«ageã€cityç­‰å­—æ®µ',
        expected_result=expected_result,
        agent_optimization=agent_optimization,
        agent_reasoning=agent_reasoning,
        scoring_rules=scoring_rules
    )
    
    try:
        headers = {
            'Authorization': f'Bearer {os.getenv("EVAL_LLM_API_KEY")}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': os.getenv('EVAL_LLM_MODEL', 'gpt-4'),
            'messages': [{'role': 'user', 'content': prompt}],
            'max_tokens': 300,
            'temperature': 0.1
        }
        
        response = requests.post(
            f"{os.getenv('EVAL_LLM_BASE_URL', 'https://api.openai.com/v1')}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        response.raise_for_status()
        result = response.json()
        llm_response = result['choices'][0]['message']['content']
        
        # è§£æJSONå“åº”
        eval_outcome = json.loads(llm_response)
        return eval_outcome
        
    except Exception as e:
        logging.error(f"LLMè¯„åˆ†å¤±è´¥: {e}")
        return {"score": 0, "explanation": f"è¯„åˆ†å¤±è´¥: {str(e)}"}


def load_scenario_weights():
    """ä»metaæ–‡ä»¶è¯»å–åœºæ™¯æƒé‡"""
    weights = {}
    try:
        with open('datasets/meta.txt', 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    name, score = line.split('=')
                    weights[name.strip()] = int(score.strip())
    except FileNotFoundError:
        logging.warning("æœªæ‰¾åˆ° datasets/meta.txt æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
        weights = {'example': 40, 'index_invalidation': 30, 'inefficient_join': 30}
    return weights

def calculate_final_scores(results):
    """è®¡ç®—æœ€ç»ˆå¾—åˆ†"""
    scenario_weights = load_scenario_weights()
    
    # æŒ‰åœºæ™¯åˆ†ç»„ç»Ÿè®¡
    scenario_stats = {}
    for result in results:
        dataset = result['dataset']
        if dataset not in scenario_stats:
            scenario_stats[dataset] = {'scores': [], 'questions': []}
        scenario_stats[dataset]['scores'].append(result['score'])
        scenario_stats[dataset]['questions'].append(result)
    
    # è®¡ç®—æ¯ä¸ªåœºæ™¯çš„å¾—åˆ†
    final_scores = {}
    for dataset, stats in scenario_stats.items():
        weight = scenario_weights.get(dataset, 0)
        avg_score = sum(stats['scores']) / len(stats['scores']) / 10  # è½¬æ¢ä¸º0-1
        final_scores[dataset] = {
            'weight': weight,
            'actual_score': weight * avg_score,
            'question_count': len(stats['scores']),
            'avg_per_question': weight / len(stats['scores'])
        }
    
    return final_scores, sum(s['actual_score'] for s in final_scores.values())

def generate_summary_table(results):
    """ç”Ÿæˆæ±‡æ€»è¡¨æ ¼"""
    from datetime import datetime
    
    scenario_weights = load_scenario_weights()
    final_scores, total_final_score = calculate_final_scores(results)
    
    print(f"\nğŸ“Š SQL Agent è¯„ä¼°æ±‡æ€»è¡¨æ ¼")
    print(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 90)
    
    # è¡¨å¤´
    print(f"{'è¯„æµ‹é›†':<18} {'è¯„æµ‹æ¡ˆä¾‹':<25} {'é—®é¢˜åˆ†æ•°':<12} {'åœºæ™¯å¾—åˆ†':<15}")
    print("-" * 90)
    
    # æŒ‰æ•°æ®é›†åˆ†ç»„æ˜¾ç¤º
    for dataset in scenario_weights.keys():
        dataset_results = [r for r in results if r['dataset'] == dataset]
        if not dataset_results:
            continue
            
        final_info = final_scores[dataset]
        per_question_weight = final_info['avg_per_question']
        
        # æ˜¾ç¤ºç¬¬ä¸€è¡Œï¼ˆåŒ…å«æ•°æ®é›†åç§°å’Œåœºæ™¯å¾—åˆ†ï¼‰
        first_result = dataset_results[0]
        first_title = first_result['title'][:22] + "..." if len(first_result['title']) > 25 else first_result['title']
        first_score = f"{first_result['score']}/10"
        first_weighted = f"({per_question_weight:.1f}åˆ†)"
        scenario_score = f"{final_info['actual_score']:.1f}/{final_info['weight']}"
        
        print(f"{dataset:<18} {first_title:<25} {first_score:<8}{first_weighted:<4} {scenario_score}")
        
        # æ˜¾ç¤ºå…¶ä»–é—®é¢˜
        for result in dataset_results[1:]:
            title_short = result['title'][:22] + "..." if len(result['title']) > 25 else result['title']
            score_display = f"{result['score']}/10"
            weighted_display = f"({per_question_weight:.1f}åˆ†)"
            print(f"{'':<18} {title_short:<25} {score_display:<8}{weighted_display:<4} {''}")
        
        print("-" * 90)
    
    # æ€»è®¡è¡Œ
    print(f"{'æ€»è®¡':<18} {f'{len(results)}ä¸ªé—®é¢˜':<25} {'':<12} {total_final_score:.1f}/100")
    print("=" * 90)

def generate_detailed_report(results):
    """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šåˆ°markdownæ–‡ä»¶"""
    from datetime import datetime
    import os
    
    # ç¡®ä¿reportsç›®å½•å­˜åœ¨
    os.makedirs('reports', exist_ok=True)
    
    report_content = f"""# SQL Agent è¯„ä¼°è¯¦ç»†æŠ¥å‘Š

**è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**è¯„ä¼°é—®é¢˜æ€»æ•°**: {len(results)}  
**æ€»åˆ†**: {sum(r['score'] for r in results)}/{len(results) * 10}  
**å¹³å‡åˆ†**: {sum(r['score'] for r in results) / len(results):.1f}/10  

---

"""
    
    # æŒ‰æ•°æ®é›†åˆ†ç»„
    datasets = {}
    for result in results:
        dataset = result['dataset']
        if dataset not in datasets:
            datasets[dataset] = []
        datasets[dataset].append(result)
    
    for dataset_name, dataset_results in datasets.items():
        dataset_score = sum(r['score'] for r in dataset_results)
        dataset_avg = dataset_score / len(dataset_results)
        
        report_content += f"""## æ•°æ®é›†: {dataset_name}

**å¾—åˆ†**: {dataset_score}/{len(dataset_results) * 10} (å¹³å‡: {dataset_avg:.1f}/10)  
**é—®é¢˜æ•°**: {len(dataset_results)}

"""
        
        for i, result in enumerate(dataset_results, 1):
            report_content += f"""### é—®é¢˜ {i}: {result['title']}

**SQLæŸ¥è¯¢**:
```sql
{result['sql']}
```

**æœŸæœ›ç»“æœ**: {result['expected']}

**Agentå›ç­”**:
- **ä¼˜åŒ–å»ºè®®**: {result['agent_optimization'] or 'æ— å»ºè®®'}
- **ç†ç”±è¯´æ˜**: {result['agent_reasoning']}

**è¯„åˆ†ç»“æœ**: {result['score']}/10

**è¯„åˆ†ç†ç”±**: {result['explanation']}

---

"""
    
    # å†™å…¥æ–‡ä»¶
    with open('reports/evaluation_report.md', 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    logging.info("è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° reports/evaluation_report.md")

def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # ç¡®ä¿logsç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/eval.log'),
            logging.StreamHandler()
        ]
    )
    # è®¾ç½®ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('requests').setLevel(logging.WARNING)


async def main():
    """ä¸»å‡½æ•°"""
    setup_logging()
    
    print("ğŸš€ SQL Agent è¯„ä¼°å¼€å§‹")
    
    # 1. æ‰«æå¯ç”¨æ•°æ®é›†
    load_dotenv('config/.env')
    datasets = scan_datasets()
    
    if not datasets:
        print("âŒ æ²¡æœ‰å‘ç°å¯ç”¨çš„æ•°æ®é›†")
        return
    
    # 2. å»ºç«‹æ•°æ®åº“è¿æ¥ï¼ˆä¸€æ¬¡æ€§è¿æ¥ï¼‰
    logging.info("å»ºç«‹æ•°æ®åº“è¿æ¥...")
    config = {
        'host': os.getenv('DATABASE_HOST', 'localhost'),
        'port': int(os.getenv('DATABASE_PORT', 3306)),
        'user': os.getenv('DATABASE_USER', 'root'),
        'password': os.getenv('DATABASE_PASSWORD', ''),
        'database': os.getenv('DATABASE_NAME', 'sql_eval_test')
    }
    conn = pymysql.connect(**config)
    logging.info(f"æ•°æ®åº“è¿æ¥æˆåŠŸ: {config['host']}:{config['port']}")
    
    # 3. éå†æ‰€æœ‰æ•°æ®é›†è¿›è¡Œè¯„ä¼°
    results = []
    all_total_score = 0
    all_question_count = 0
    
    for dataset in datasets:
        logging.info(f"å¼€å§‹è¯„ä¼°æ•°æ®é›†: {dataset['name']}")
        
        # æ„å»ºæ•°æ®åº“ç¯å¢ƒï¼ˆå¤ç”¨è¿æ¥ï¼‰
        setup_database_with_connection(conn, dataset['schema_file'], dataset['data_file'])
        
        # è§£æé—®é¢˜
        questions = parse_questions(Path(dataset['questions_file']))
        
        # è¿è¡Œè¯„ä¼°
        dataset_total_score = 0
        
        for i, q in enumerate(questions, 1):
            # Agentå¤„ç†
            agent_result = mock_agent(q['sql'])
            
            # LLMè¯„åˆ†
            eval_result = await evaluate_with_llm(agent_result, q['expected'], q['scoring_rules'], q['sql'])
            score = eval_result['score']
            
            # æ”¶é›†ç»“æœ
            results.append({
                'dataset': dataset['name'],
                'question_id': i,
                'title': q['title'],
                'sql': q['sql'],
                'expected': q['expected'],
                'agent_optimization': agent_result['optimization'],
                'agent_reasoning': agent_result['reasoning'],
                'score': score,
                'explanation': eval_result['explanation'].strip()
            })
            
            dataset_total_score += score
        
        all_total_score += dataset_total_score
        all_question_count += len(questions)
        
        logging.info(f"æ•°æ®é›† {dataset['name']} è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {dataset_total_score}/{len(questions) * 10}")
    
    conn.close()
    
    # 4. ç”Ÿæˆæ±‡æ€»è¡¨æ ¼å’Œè¯¦ç»†æŠ¥å‘Š
    generate_summary_table(results)
    generate_detailed_report(results)
    
    print(f"\nğŸ† æ€»ä½“è¯„ä¼°å®Œæˆ!")
    print(f"æ€»åˆ†: {all_total_score}/{all_question_count * 10}")
    print(f"æ€»å¹³å‡åˆ†: {all_total_score / all_question_count:.1f}/10")
    print(f"ğŸ“Š æ±‡æ€»è¡¨æ ¼å·²æ˜¾ç¤ºåœ¨æ§åˆ¶å°")
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° reports/evaluation_report.md")


if __name__ == "__main__":
    asyncio.run(main())